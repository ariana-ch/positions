\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{titlesec}

% Setup
\geometry{margin=1in}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

% Section Formatting
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}

\title{\textbf{Project Specification: Client Position Forecasting via Dynamic Graphs}}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    This document outlines the technical specification for predicting client entry and exit points in financial assets. It defines the problem as a link prediction task on a dynamic bipartite graph and provides a theoretical framework for handling temporal dependencies. Specifically, it details the mathematical formulation for State-Dependent Modeling using Temporal Graph Networks.
\end{abstract}

\section{Problem Definition}
The objective is to predict the future interactions between \textbf{Clients} and \textbf{Stocks}. In financial markets, a ``position'' is not a static property; it is a temporal event with a distinct start (Entry) and end (Exit).

We formally define the ecosystem as a \textbf{Dynamic Bipartite Graph} $G(t) = (U, V, E(t))$, where:
\begin{itemize}
    \item $U$: Set of Client nodes (investors).
    \item $V$: Set of Stock nodes (assets).
    \item $E(t)$: Set of edges active at time $t$. An edge $(u, v)$ represents an open position.
\end{itemize}

The core challenge is processing the \textbf{sequence of events} that alters the graph structure, rather than analyzing static snapshots. This requires a model capable of learning from the evolution of interactions over continuous time.

\section{Temporal State Representation}

To model the path-dependent nature of financial behavior, this specification proposes a \textbf{Memory Module} architecture. This approach aligns with frameworks such as JODIE \cite{kumar2019} and TGN \cite{rossi2020}, which allow the model to maintain a persistent state for each user, capturing historical context (e.g., past volatility experience or trading frequency) beyond immediate neighbors.

\subsection{Mathematical Formulation}
Every node $u$ maintains a memory state $S_u(t)$. This state serves as a compressed representation of the user's historical behavior up to time $t$. The update mechanism operates in three steps for every batch of interactions:

\subsubsection{Step 1: Message Passing (The Interaction)}
When an event occurs (e.g., Client $u$ buys Stock $v$ at time $t$), a ``message'' is generated. This message captures the context of the interaction:
\begin{equation}
    m_u(t) = \text{MSG}(S_u(t^-), S_v(t^-), \Delta t, \mathbf{e}_{uv})
\end{equation}
Where:
\begin{itemize}
    \item $S_u(t^-)$: The user's memory state just before the trade.
    \item $\Delta t$: Time elapsed since the user's last action.
    \item $\mathbf{e}_{uv}$: Edge features (e.g., transaction value, Unrealized P\&L).
\end{itemize}

\subsubsection{Step 2: Memory Update (The State Transition)}
The node's memory is updated using a Recurrent Neural Network (e.g., GRU). This enables the system to update the user's profile in real-time without retraining on the full history:
\begin{equation}
    S_u(t) = \text{GRU}(m_u(t), S_u(t^-))
\end{equation}

\subsubsection{Step 3: Temporal Embedding (The Prediction)}
To predict a future link (Entry) or link deletion (Exit), a temporal embedding $Z_u(t)$ is generated. This combines the user's internal memory with the current graph structure (their current portfolio):
\begin{equation}
    Z_u(t) = \sum_{j \in \mathcal{N}(u)} h(S_u(t), S_j(t), \mathbf{e}_{uj})
\end{equation}
Where $\mathcal{N}(u)$ represents the set of stocks currently held by the user.

\section{Implementation Roadmap}
The project execution is divided into three distinct phases.

\subsection{Phase 1: Feature Definition \& Data Sourcing}
Before constructing the graph, the input features that drive the memory updates must be defined. This phase focuses on aggregating three data sources:
\begin{itemize}
    \item \textbf{Client Features (Node $U$):} Static data (Risk Profile, AUM) and Historical Transaction Logs (to calculate past turnover rates).
    \item \textbf{Stock Features (Node $V$):} Dynamic time-series data including Price History (OHLC), Trading Volume, and Volatility metrics.
    \item \textbf{Interaction Features (Edge $E$):} Transaction-specific data such as Trade Size, Direction (Buy/Sell), and calculated Unrealized P\&L at the time of the event.
\end{itemize}

\subsection{Phase 2: Static Graph Baseline}
To justify the complexity of a Temporal Graph Network, a baseline must be established using a static Graph Neural Network (e.g., GraphSAGE or GCN).
\begin{itemize}
    \item \textbf{Methodology:} The data will be treated as discrete daily snapshots. For each day $t$, a static graph $G_t$ is constructed representing all active positions. A GNN is trained to predict the existence of edges in $G_{t+1}$ based solely on the structure of $G_t$.
    \item \textbf{Justification:} This isolates the value of \textit{connectivity} from the value of \textit{time}. If the Static GNN performs well, it indicates that ``what you hold'' (network structure) is predictive. If it performs poorly compared to Phase 3, it proves that ``when you acted'' (temporal sequence) is the dominant factor \cite{feng2019}.
\end{itemize}

\subsection{Phase 3: Temporal Graph Implementation (TGN)}
This phase involves the full implementation of the memory-based architecture described in Section 2.
\begin{itemize}
    \item \textbf{Event Stream Processing:} Unlike the baseline, this model will ingest data as a continuous stream of events $(u, v, t)$.
    \item \textbf{Memory Module Integration:} Implement the GRU-based state updater. This requires defining the ``Message Function'' that compresses the features from Phase 1 into the hidden state.
    \item \textbf{Multi-Head Training:} The output layer will be split into two distinct classifiers trained simultaneously:
    \begin{enumerate}
        \item \textit{Entry Head:} trained using Negative Sampling (random unlinked stocks).
        \item \textit{Exit Head:} trained specifically on existing edges, using Unrealized P\&L as a primary signal.
    \end{enumerate}
\end{itemize}

\begin{thebibliography}{9}

\bibitem{feng2019}
F. Feng, X. He, et al.
\newblock \href{https://dl.acm.org/doi/10.1145/3309547}{Temporal Relational Ranking for Stock Prediction}.
\newblock \textit{ACM Transactions on Information Systems (TOIS)}, 37(2), 2019.

\bibitem{kumar2019}
S. Kumar, X. Zhang, and J. Leskovec.
\newblock \href{https://arxiv.org/abs/1908.01207}{JODIE: Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks}.
\newblock \textit{Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining (KDD '19)}, 2019.

\bibitem{rossi2020}
E. Rossi, et al.
\newblock \href{https://arxiv.org/abs/2006.10637}{Temporal Graph Networks for Deep Learning on Dynamic Graphs}.
\newblock \textit{arXiv preprint arXiv:2006.10637}, 2020.

\bibitem{takahashi2024}
S. Takahashi, et al.
\newblock \href{https://arxiv.org/abs/2409.08718}{Dynamic Link and Flow Prediction in Bank Transfer Networks}.
\newblock \textit{arXiv preprint arXiv:2409.08718}, 2024.

\bibitem{najafabadipour2025}
Z. Najafabadipour, et al.
\newblock \href{https://cmde.tabrizu.ac.ir/article_20783.html?lang=en}{Predicting Retail Investor Behavior using Dynamic Graph Neural Networks}.
\newblock \textit{Computational Methods for Differential Equations}, 13(2), 2025.

\end{thebibliography}

\end{document}
